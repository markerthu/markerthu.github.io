---
permalink: /
title: "Jiajun Fan"
excerpt: "CS Ph.D. Student at UIUC | Reinforcement Learning Researcher"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<style>
.news-item { margin-bottom: 6px; }
.pub-entry { margin-bottom: 14px; padding: 10px 14px; border-left: 3px solid #4a90d9; background: #f8f9fa; border-radius: 0 6px 6px 0; }
.pub-entry .venue { color: #666; font-size: 0.9em; }
.pub-badge { display: inline-block; padding: 2px 7px; border-radius: 4px; font-size: 0.78em; font-weight: bold; margin-right: 4px; }
.badge-oral { background: #d4edda; color: #155724; }
.badge-conf { background: #cce5ff; color: #004085; }
.badge-journal { background: #fff3cd; color: #856404; }
.section-title { border-bottom: 2px solid #4a90d9; padding-bottom: 4px; margin-top: 28px; }
.research-card { padding: 10px 14px; margin-bottom: 10px; border-radius: 6px; background: #f0f4ff; }
</style>

I am a Computer Science Ph.D. student at the **University of Illinois Urbana-Champaign (UIUC)**, working at the intersection of **reinforcement learning theory and large-scale AI systems**. My research focuses on developing **self-evolving AI systems** that can learn continuously from human/AI feedback while maintaining reliability and sample efficiency.

> ğŸ“ **Currently seeking research internship opportunities for Summer 2026.** &nbsp; [[CV]](files/CV.pdf) &nbsp; [[Google Scholar]](https://scholar.google.com/citations?user=EjmzseUAAAAJ&hl=en)

---

## ğŸ“° Latest News {#news}

<div class="news-item">ğŸ† <strong>[2026-01]</strong> Two papers accepted at <strong>ICLR 2026</strong> â€” See you in Rio de Janeiro!</div>
<div class="news-item">ğŸ‰ <strong>[2025-09]</strong> Two papers accepted at <strong>NeurIPS 2025</strong> â€” See you in San Diego!</div>
<div class="news-item">ğŸ“„ <strong>[2025-06]</strong> Paper accepted at <strong>TPAMI</strong> (PRANCE: Adaptive ViT Inference).</div>
<div class="news-item">âœ… <strong>[2025-02]</strong> Paper on self-evolving Flow Matching accepted at <strong>ICLR 2025</strong>.</div>
<div class="news-item">ğŸ‘¥ <strong>[2025-01]</strong> Selected as reviewer for ICML 2025, ICLR 2025, and NeurIPS 2024.</div>
<div class="news-item">ğŸ“ <strong>[2024-08]</strong> Started Ph.D. at UIUC; research on collapse-free self-evolution begins.</div>

---

## ğŸ“š Selected Publications {#publications}

*Full list on [Google Scholar](https://scholar.google.com/citations?user=EjmzseUAAAAJ&hl=en) and the [Publications](/publications/) page.*

<div class="pub-entry">
<span class="pub-badge badge-conf">ICLR 2026</span>
<strong>Incentivizing Consistent, Effective and Scalable Reasoning Capability in Audio LLMs via Reasoning Process Rewards</strong><br>
<span class="venue">J. Fan*, R. Ren, J. Li, R. Pandey, P.G. Shivakumar, Y. Gu, A. Gandhe, G. Liu, I. Bulyko &nbsp;Â·&nbsp; ICLR 2026</span><br>
Proposed CESAR: online RL with multi-faceted process rewards that resolves test-time inverse scaling in Audio LLMs. Achieves SOTA on MMAU, outperforming Gemini 2.5 Pro and GPT-4o Audio.
</div>

<div class="pub-entry">
<span class="pub-badge badge-conf">NeurIPS 2025</span>
<strong>Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models</strong><br>
<span class="venue">J. Fan*, et al. &nbsp;Â·&nbsp; NeurIPS 2025</span><br>
Adaptive KL control for collapse-free, continuous post-training of generative models â€” no manual divergence tuning required.
</div>

<div class="pub-entry">
<span class="pub-badge badge-conf">ICLR 2025</span>
<strong>Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein Regularization</strong><br>
<span class="venue">J. Fan*, et al. &nbsp;Â·&nbsp; ICLR 2025</span><br>
First theoretically-grounded framework for continuous self-evolution of flow matching models. Enables training without human-collected data; validated on Stable Diffusion 3.
</div>

<div class="pub-entry">
<span class="pub-badge badge-journal">TPAMI 2025</span>
<strong>PRANCE: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference</strong><br>
<span class="venue">Y. Li, C. Tang, Y. Meng, J. Fan, et al. &nbsp;Â·&nbsp; IEEE TPAMI 2025</span>
</div>

<div class="pub-entry">
<span class="pub-badge badge-oral">ICLR 2023 Oral</span>
<strong>Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection</strong><br>
<span class="venue">J. Fan*, et al. &nbsp;Â·&nbsp; ICLR 2023 â€” Oral Presentation (Top 5 / 4176)</span><br>
First demonstration of MoE advantages in RL. Broke 24 human world records with 500Ã— less data than prior SOTA.
</div>

<div class="pub-entry">
<span class="pub-badge badge-conf">ICML 2022</span>
<strong>Generalized Data Distribution Iteration</strong><br>
<span class="venue">J. Fan*, C. Xiao &nbsp;Â·&nbsp; ICML 2022</span><br>
First theoretical proof of the importance of data distribution optimization in RL. Outperformed Agent57 with 500Ã— less data.
</div>

---

## ğŸ”¬ Research Focus {#research}

<div class="research-card">
<strong>ğŸ¤– Self-Evolving AI Systems</strong> â€” Creating AI that continuously improves online via RL while preventing collapse and maintaining diversity.
</div>
<div class="research-card">
<strong>âš¡ Data-Efficient Learning</strong> â€” Algorithms achieving superhuman performance with orders of magnitude less data.
</div>
<div class="research-card">
<strong>ğŸ“ Theoretical Foundations</strong> â€” Rigorous mathematical frameworks for understanding and guaranteeing AI learning.
</div>

---

## ğŸ“¬ Contact

- **Email:** [jiajunf3@illinois.edu](mailto:jiajunf3@illinois.edu)
- **Office:** Thomas M. Siebel Center for Computer Science, UIUC
- **Links:** [CV](files/CV.pdf) Â· [Google Scholar](https://scholar.google.com/citations?user=EjmzseUAAAAJ&hl=en) Â· [GitHub](https://github.com/markerthu) Â· [LinkedIn](https://www.linkedin.com/in/jiajun-fan-57b12b26b)

---

> *"The goal is not just to build better AI, but to understand intelligence itself."*
