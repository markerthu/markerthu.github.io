---
layout: null
---
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>ADRPO ‚Äî Adaptive Divergence | NeurIPS 2025</title>
<meta name="description" content="Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models. NeurIPS 2025.">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
<style>
*{box-sizing:border-box;margin:0;padding:0}
body{font-family:'Inter',sans-serif;background:#fff;color:#1a1a2e;line-height:1.7}
a{color:#6a1b9a;text-decoration:none}a:hover{text-decoration:underline}

.topnav{background:#fff;border-bottom:1.5px solid #e0e8f0;padding:12px 24px;display:flex;align-items:center;gap:12px;font-size:0.84em;box-shadow:0 1px 4px rgba(0,0,0,0.05)}
.topnav a{color:#1565c0;font-weight:600}
.topnav span{color:#bbb}


  50%  { background-position: 100% 50%; }
  100% { background-position: 0% 50%;   }
}
.hero{background:#f8fbff;border-bottom:1.5px solid #e0e8f0;padding:52px 24px 44px;text-align:center}
.venue-badge{display:inline-block;background:#7b1fa2;color:#fff;padding:6px 18px;border-radius:24px;font-size:0.82em;font-weight:800;letter-spacing:0.08em;text-transform:uppercase;margin-bottom:20px}
.hero h1{font-size:clamp(1.5em,4vw,2.4em);font-weight:800;color:#1a2332;line-height:1.25;max-width:800px;margin:0 auto 14px}
.hero .short{font-size:1.05em;color:#555;margin-bottom:28px;font-weight:500}
.authors{font-size:0.88em;color:#444;max-width:660px;margin:0 auto 20px;line-height:1.9}
.authors strong{color:#1a2332;font-weight:700}
.affil{font-size:0.8em;color:#666;margin-bottom:28px}
.btn-row{display:flex;flex-wrap:wrap;gap:10px;justify-content:center}
.btn{display:inline-flex;align-items:center;gap:6px;padding:9px 20px;border-radius:8px;font-size:0.85em;font-weight:700;transition:opacity .2s}
.btn-paper{background:#fff;color:#1565c0;border:1.5px solid #1565c0}
.btn-arxiv{background:#e53935;color:#fff}
.btn-home{background:#546e7a;color:#fff}
.btn:hover{opacity:.85}

.container{max-width:860px;margin:0 auto;padding:0 24px}
section{padding:56px 0 44px}
section+section{border-top:1px solid #f0f0f0}
section p{text-align:justify}
h2{font-size:1.42em;font-weight:800;color:#1a1a2e;margin-bottom:18px;display:flex;align-items:center;gap:10px}
h2::after{content:'';flex:1;height:2px;background:linear-gradient(to right,#7b1fa2,transparent);border-radius:1px}
p{color:#333;margin-bottom:14px;font-size:0.97em}

.tldr{background:#f3e5f5;border-left:4px solid #7b1fa2;border-radius:0 10px 10px 0;padding:16px 22px}
.tldr strong{color:#4a148c;font-weight:800}

/* Key insight visual */
.insight-box{background:linear-gradient(135deg,#f3e5f5,#e1bee7);border-radius:12px;padding:20px 24px;margin:20px 0;display:flex;gap:20px;align-items:flex-start}
.insight-icon{font-size:2.2em;flex-shrink:0}
.insight-text h3{font-size:0.95em;font-weight:800;color:#4a148c;margin-bottom:6px}
.insight-text p{font-size:0.87em;color:#444;margin:0;line-height:1.6}

/* Comparison */
.vs-grid{display:grid;grid-template-columns:1fr 1fr;gap:16px;margin:20px 0}
.vs-card{border-radius:10px;padding:18px}
.vs-bad{background:#fff3e0;border:2px solid #ffcc80}
.vs-good{background:#e8f5e9;border:2px solid #a5d6a7}
.vs-card h3{font-size:0.88em;font-weight:800;margin-bottom:8px}
.vs-bad h3{color:#e65100}
.vs-good h3{color:#2e7d32}
.vs-card ul{font-size:0.82em;color:#444;padding-left:1.2em;line-height:1.9;margin:0}

/* Results */
.results-grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(150px,1fr));gap:14px;margin:20px 0}
.result-card{background:linear-gradient(135deg,#4a148c,#7b1fa2);border-radius:12px;padding:16px;text-align:center}
.result-card .rnum{font-size:1.7em;font-weight:900;color:#fff;line-height:1.1}
.result-card .rdesc{font-size:0.75em;color:#e1bee7;margin-top:6px;line-height:1.4}

.tag{display:inline-block;padding:2px 10px;border-radius:20px;font-size:0.78em;font-weight:700;margin:3px 3px 0 0}
.tag-gen{background:#e1bee7;color:#4a148c}
.tag-llm{background:#e3f2fd;color:#0d47a1}
.tag-audio{background:#e8f5e9;color:#2e7d32}

footer{background:#f5f7fa;color:#666;text-align:center;padding:24px;font-size:.84em;border-top:1px solid #e0e8f0}
footer a{color:#1565c0}

.fig-grid-2{display:grid;grid-template-columns:1fr 1fr;gap:16px;margin-bottom:16px}
@media(max-width:600px){.vs-grid{grid-template-columns:1fr}.results-grid{grid-template-columns:1fr 1fr}.fig-grid-2{grid-template-columns:1fr}}
</style>
</head>
<body>

<div class="topnav">
  <a href="/">‚Üê Jiajun Fan</a>
  <span>/</span>
  <a href="/publications/">Publications</a>
  <span>/</span>
  <span style="color:#c9a8e0;">ADRPO</span>
</div>

<div class="hero">
  <div class="venue-badge">üéâ NeurIPS 2025</div>
  <h1>Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models</h1>
  <p class="short"><strong>ADRPO</strong>: Sample-level adaptive divergence control ‚Äî no manual tuning required</p>
  <div class="authors">
    <strong><a href="/" style="color:#fff">Jiajun Fan</a></strong>&thinsp;<sup>1</sup>,
    <a href="#" style="color:#c9a8e0">Tong Wei</a>&thinsp;<sup>1</sup>,
    <a href="#" style="color:#c9a8e0">Chaoran Cheng</a>&thinsp;<sup>1</sup>,
    <a href="#" style="color:#c9a8e0">Yuxin Chen</a>&thinsp;<sup>1</sup>,
    <a href="https://geliu.web.illinois.edu/" style="color:#c9a8e0">Ge Liu</a>&thinsp;<sup>1</sup>
  </div>
  <div class="affil"><sup>1</sup>University of Illinois Urbana-Champaign</div>
  <div class="btn-row">
    <a class="btn btn-paper" href="https://openreview.net/forum?id=aXO0xg0ttW">üìÑ Paper (OpenReview)</a>
    <a class="btn btn-arxiv" href="https://arxiv.org/abs/2510.18053">arXiv</a>
    <a class="btn btn-home" href="/">üè† Author Homepage</a>
  </div>
</div>

<div class="container">

<section>
  <div class="tldr">
    <strong>TL;DR</strong> ‚Äî Fixed divergence regularization in RLHF creates a dilemma: strong regularization preserves diversity but limits alignment; weak regularization enables better alignment but risks collapse. ADRPO <strong>automatically adapts</strong> the regularization strength at the sample level based on advantage estimates ‚Äî high-quality samples get freedom to explore, poor samples get stronger constraints. Works with W2 (flow matching) and KL (LLMs).
  </div>
</section>

<!-- Teaser Figure -->
<section style="padding:20px 0 30px;border:none">
  <figure style="margin:0">
    <img src="images/teaser.png" alt="ADRPO Qualitative Results ‚Äî 2B vs 12B"
         style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
    <figcaption style="text-align:center;font-size:0.82em;color:#666;margin-top:12px;line-height:1.6">
      <strong>Figure 1.</strong> Qualitative comparison. Our 2B SD3 model fine-tuned with ADRPO matches or outperforms FLUX.1-Dev (12B) and SANA-1.5 (4.8B) on artistic style rendering, attribute binding, coloring, and compositional control ‚Äî with 2‚Äì6√ó fewer parameters.
    </figcaption>
  </figure>
</section>

<section>
  <h2>üí° Key Insight</h2>
  <div class="insight-box">
    <div class="insight-icon">‚öñÔ∏è</div>
    <div class="insight-text">
      <h3>Not all samples should be treated equally</h3>
      <p>Existing RLHF methods apply the same divergence regularization to every sample. ADRPO observes that <strong>high-advantage samples</strong> (clearly better than baseline) deserve <strong>less regularization</strong> to fully exploit their quality, while <strong>low-advantage samples</strong> need <strong>stronger regularization</strong> to prevent harmful policy updates. This simple principle unlocks significantly better exploration-exploitation trade-offs.</p>
    </div>
  </div>
  <div class="vs-grid">
    <div class="vs-card vs-bad">
      <h3>‚ùå Fixed Regularization (ORW-CFM-W2 / PPO)</h3>
      <ul>
        <li>Same regularization for all samples</li>
        <li>Strong KL ‚Üí conservative, slow progress</li>
        <li>Weak KL ‚Üí reward hacking / collapse</li>
        <li>Manual KL coefficient tuning required</li>
      </ul>
    </div>
    <div class="vs-card vs-good">
      <h3>‚úÖ ADRPO (Ours)</h3>
      <ul>
        <li>Adaptive KL based on advantage estimates</li>
        <li>High-value samples explore more freely</li>
        <li>Poor samples kept on a short leash</li>
        <li>Plug-and-play ‚Äî no extra networks</li>
      </ul>
    </div>
  </div>
</section>

<section>
  <h2>üåê Generalization</h2>
  <p>ADRPO is a universal plug-in that generalizes across <strong>different model types and modalities</strong>:</p>
  <p>
    <span class="tag tag-gen">üåä Flow/Diffusion Models (SD3)</span>
    <span class="tag tag-llm">üß† Text LLMs (GRPO)</span>
    <span class="tag tag-audio">üéµ Audio LLMs</span>
  </p>
  <p style="margin-top:14px;">In LLM fine-tuning, ADRPO shows an <strong>emergent ability to escape local optima</strong> through active exploration. In multimodal audio reasoning, it outperforms GRPO through superior step-by-step reasoning.</p>
</section>

<section>
  <h2>üìä Results</h2>
  <div class="results-grid">
    <div class="result-card">
      <div class="rnum">2B</div>
      <div class="rdesc">SD3 model surpasses<br>4.8B &amp; 12B models</div>
    </div>
    <div class="result-card">
      <div class="rnum">&gt;DPO</div>
      <div class="rdesc">Beats offline DPO<br>in alignment &amp; diversity</div>
    </div>
    <div class="result-card">
      <div class="rnum">&gt;ORW</div>
      <div class="rdesc">Beats ORW-CFM-W2<br>(fixed regularization)</div>
    </div>
    <div class="result-card">
      <div class="rnum">0</div>
      <div class="rdesc">Extra networks needed<br>(plug-and-play)</div>
    </div>
  </div>
  <p style="font-size:0.88em;color:#666;">Evaluated on text-to-image generation tasks: attribute binding, semantic consistency, artistic style transfer, and compositional control.</p>

  <figure style="margin:24px 0 0">
    <img src="images/comparison.png" alt="ADRPO vs other RL methods"
         style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
    <figcaption style="text-align:center;font-size:0.82em;color:#666;margin-top:12px;line-height:1.6">
      <strong>Figure 2.</strong> Comparison with RL fine-tuning baselines (DPO, ORW-CFM-W2). ADRPO achieves superior style fidelity, spatial reasoning, and attribute binding.
    </figcaption>
  </figure>
</section>

<!-- Reward-KL Tradeoff -->
<section>
  <h2>üìà Reward‚ÄìDiversity Tradeoff</h2>
  <figure style="margin:0">
    <img src="images/reward_kl.png" alt="Reward vs KL tradeoff"
         style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
    <figcaption style="text-align:center;font-size:0.82em;color:#666;margin-top:10px;line-height:1.6">
      <strong>Figure 3.</strong> Reward vs. KL divergence tradeoff. ADRPO achieves <em>higher reward at lower KL</em> than fixed-regularization baselines ‚Äî demonstrating that adaptive control finds a better exploration‚Äìexploitation balance.
    </figcaption>
  </figure>
</section>

<!-- Reward-Diversity Analysis -->
<section>
  <h2>‚öñÔ∏è Reward vs. Diversity Analysis</h2>
  <p style="font-size:0.88em;color:#555;margin-bottom:16px;">The core insight: fixed-KL methods force a single regularization strength across all samples. ADRPO adaptively adjusts ‚Äî easy samples get tighter control, hard samples get more freedom.</p>
  <div class="fig-grid-2">
    <figure style="margin:0">
      <img src="images/reward_vs_diversity.png" alt="Reward vs diversity"
           style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
      <figcaption style="text-align:center;font-size:0.78em;color:#666;margin-top:8px">
        <strong>Figure 4.</strong> Reward vs. diversity Pareto front. ADRPO achieves <strong>higher reward at any given diversity level</strong>.
      </figcaption>
    </figure>
    <figure style="margin:0">
      <img src="images/reward_vs_kl.png" alt="Reward vs KL"
           style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
      <figcaption style="text-align:center;font-size:0.78em;color:#666;margin-top:8px">
        <strong>Figure 5.</strong> Reward vs. KL divergence. ADRPO achieves the same reward at <strong>lower KL</strong> ‚Äî more efficient exploration.
      </figcaption>
    </figure>
  </div>
</section>

<!-- More Qualitative Results -->
<section>
  <h2>üé® More Qualitative Results</h2>
  <figure style="margin:0 0 20px">
    <img src="images/app_compare_models.png" alt="Extended comparison with larger models"
         style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
    <figcaption style="text-align:center;font-size:0.82em;color:#666;margin-top:10px;line-height:1.6">
      <strong>Figure 4.</strong> Extended comparison ‚Äî ADRPO's 2B SD3 vs. FLUX.1-Dev (12B) and SANA-1.5 (4.8B) on additional prompts. ADRPO consistently matches or outperforms models with 2‚Äì6√ó more parameters.
    </figcaption>
  </figure>
  <figure style="margin:0 0 20px">
    <img src="images/app_compare_methods.png" alt="Extended comparison with RL methods"
         style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
    <figcaption style="text-align:center;font-size:0.82em;color:#666;margin-top:10px;line-height:1.6">
      <strong>Figure 5.</strong> Extended RL method comparison ‚Äî ADRPO vs. DPO and ORW-CFM-W2 on diverse T2I tasks.
    </figcaption>
  </figure>
</section>

<!-- LLM Generalization -->
<section>
  <h2>üß† Generalization to LLMs</h2>
  <p style="font-size:0.88em;color:#555;margin-bottom:16px;">ADRPO is not limited to image generation ‚Äî it generalizes to LLM fine-tuning tasks. Tested on Qwen2 and Qwen3:</p>
  <div class="fig-grid-2">
    <figure style="margin:0">
      <img src="images/llm_reward_entropy.png" alt="LLM reward vs entropy (Qwen2)"
           style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
      <figcaption style="text-align:center;font-size:0.78em;color:#666;margin-top:8px">
        <strong>Figure 8.</strong> Qwen2: ADRPO achieves higher reward while maintaining generation diversity.
      </figcaption>
    </figure>
    <figure style="margin:0">
      <img src="images/llm_qwen3_entropy.png" alt="LLM reward vs entropy (Qwen3)"
           style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
      <figcaption style="text-align:center;font-size:0.78em;color:#666;margin-top:8px">
        <strong>Figure 9.</strong> Qwen3: consistent advantage ‚Äî ADRPO's adaptive regularization generalizes across model families.
      </figcaption>
    </figure>
  </div>
</section>

<section>
  <h2>üìÖ Publication Journey</h2>
  <div style="position:relative;padding-left:24px;margin:8px 0">
    <div style="position:absolute;left:8px;top:0;bottom:0;width:2px;background:linear-gradient(to bottom,#7b1fa2,#ce93d8)"></div>
    <div style="position:relative;margin-bottom:18px;padding-left:4px">
      <div style="position:absolute;left:-20px;top:5px;width:12px;height:12px;background:#7b1fa2;border-radius:50%;border:2px solid #fff;box-shadow:0 0 0 2px #7b1fa2"></div>
      <div style="font-size:.78em;font-weight:800;color:#7b1fa2;text-transform:uppercase;letter-spacing:.05em">May 2025</div>
      <div style="font-weight:700;font-size:.93em">Submitted to NeurIPS 2025</div>
      <div style='font-size:.83em;color:#555'>Submitted to The Thirty-ninth Annual Conference on Neural Information Processing Systems.</div>
    </div>
    <div style="position:relative;margin-bottom:18px;padding-left:4px">
      <div style="position:absolute;left:-20px;top:5px;width:12px;height:12px;background:#e53935;border-radius:50%;border:2px solid #fff;box-shadow:0 0 0 2px #e53935"></div>
      <div style="font-size:.78em;font-weight:800;color:#e53935;text-transform:uppercase;letter-spacing:.05em">Oct 2025</div>
      <div style="font-weight:700;font-size:.93em">arXiv preprint released (arXiv:2510.18053)</div>
      
    </div>
    <div style="position:relative;margin-bottom:18px;padding-left:4px">
      <div style="position:absolute;left:-20px;top:5px;width:12px;height:12px;background:#1b5e20;border-radius:50%;border:2px solid #fff;box-shadow:0 0 0 2px #1b5e20"></div>
      <div style="font-size:.78em;font-weight:800;color:#1b5e20;text-transform:uppercase;letter-spacing:.05em">Sep 2025</div>
      <div style="font-weight:700;font-size:.93em">‚úÖ Accepted at NeurIPS 2025 (Poster)</div>
      <div style='font-size:.83em;color:#555'>Accepted. <a href="https://openreview.net/forum?id=aXO0xg0ttW">OpenReview</a></div>
    </div>
    <div style="position:relative;margin-bottom:18px;padding-left:4px">
      <div style="position:absolute;left:-20px;top:5px;width:12px;height:12px;background:#1b5e20;border-radius:50%;border:2px solid #fff;box-shadow:0 0 0 2px #1b5e20"></div>
      <div style="font-size:.78em;font-weight:800;color:#1b5e20;text-transform:uppercase;letter-spacing:.05em">Dec 2025</div>
      <div style="font-weight:700;font-size:.93em">Presented at NeurIPS 2025 ¬∑ San Diego, CA</div>
      
    </div>
  </div>
</section>

<section>
  <h2>üìñ BibTeX</h2>
  <pre style="background:#f9f5ff;border-radius:8px;padding:16px;font-size:0.82em;overflow-x:auto;border:1px solid #e1bee7;"><code>@inproceedings{fan2025adaptive,
  title={Adaptive Divergence Regularized Policy Optimization
         for Fine-tuning Generative Models},
  author={Jiajun Fan and Tong Wei and Chaoran Cheng
          and Yuxin Chen and Ge Liu},
  booktitle={The Thirty-ninth Annual Conference on
             Neural Information Processing Systems},
  year={2025},
  url={https://openreview.net/forum?id=aXO0xg0ttW}
}</code></pre>
</section>

</div>

<footer>
  <p>ADRPO ¬∑ NeurIPS 2025 &nbsp;|&nbsp; <a href="/">Jiajun Fan</a> ¬∑ UIUC &nbsp;|&nbsp; <a href="https://openreview.net/forum?id=aXO0xg0ttW">Paper</a></p>
</footer>
</body>
</html>
