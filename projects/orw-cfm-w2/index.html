---
layout: null
---
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>ORW-CFM-W2 ‚Äî Flow Matching RLHF | ICLR 2025</title>
<meta name="description" content="Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein Regularization. ICLR 2025.">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
<style>
*{box-sizing:border-box;margin:0;padding:0}
body{font-family:'Inter',sans-serif;background:#fff;color:#1a1a2e;line-height:1.7}
a{color:#0277bd;text-decoration:none}a:hover{text-decoration:underline}

.topnav{background:#00162b;padding:12px 24px;display:flex;align-items:center;gap:12px;font-size:0.84em}
.topnav a{color:#80cbc4;font-weight:600}
.topnav span{color:#1e4060}

@keyframes heroShift{
  0%   { background-position: 0% 50%;   }
  50%  { background-position: 100% 50%; }
  100% { background-position: 0% 50%;   }
}
.hero{background:linear-gradient(-45deg,#002171,#0d47a1,#00838f,#002171);background-size:300% 300%;animation:heroShift 16s ease infinite;padding:64px 24px 56px;text-align:center}
.venue-badge{display:inline-block;background:#01579b;color:#fff;padding:6px 18px;border-radius:24px;font-size:0.82em;font-weight:800;letter-spacing:0.08em;text-transform:uppercase;margin-bottom:20px}
.hero h1{font-size:clamp(1.4em,3.8vw,2.3em);font-weight:800;color:#fff;line-height:1.3;max-width:800px;margin:0 auto 12px}
.hero .short{font-size:1em;color:#80deea;margin-bottom:26px;font-weight:500}
.authors{font-size:0.88em;color:#b3e5fc;max-width:660px;margin:0 auto 20px;line-height:1.9}
.authors strong{color:#fff;font-weight:700}
.affil{font-size:0.8em;color:#5baac8;margin-bottom:28px}
.btn-row{display:flex;flex-wrap:wrap;gap:10px;justify-content:center}
.btn{display:inline-flex;align-items:center;gap:6px;padding:9px 20px;border-radius:8px;font-size:0.85em;font-weight:700;transition:opacity .2s}
.btn-paper{background:#fff;color:#002171}
.btn-arxiv{background:#e53935;color:#fff}
.btn-home{background:#37474f;color:#fff}
.btn:hover{opacity:.85}

.container{max-width:860px;margin:0 auto;padding:0 24px}
section{padding:52px 0 40px}
section+section{border-top:1px solid #f0f0f0}
h2{font-size:1.42em;font-weight:800;color:#1a1a2e;margin-bottom:18px;display:flex;align-items:center;gap:10px}
h2::after{content:'';flex:1;height:2px;background:linear-gradient(to right,#0277bd,transparent);border-radius:1px}
p{color:#333;margin-bottom:14px;font-size:0.97em}

.tldr{background:#e1f5fe;border-left:4px solid #0277bd;border-radius:0 10px 10px 0;padding:16px 22px}
.tldr strong{color:#01579b;font-weight:800}

/* Challenges grid */
.challenge-grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(200px,1fr));gap:14px;margin:20px 0}
.challenge-card{background:#fff3e0;border:2px solid #ffb74d;border-radius:10px;padding:16px}
.challenge-card h3{font-size:0.85em;font-weight:800;color:#e65100;margin-bottom:6px}
.challenge-card p{font-size:0.8em;color:#555;margin:0;line-height:1.5}

/* Three contributions */
.contrib-list{counter-reset:contrib;margin:20px 0}
.contrib-item{display:flex;gap:16px;margin-bottom:18px;align-items:flex-start}
.contrib-num{width:36px;height:36px;background:linear-gradient(135deg,#0277bd,#00838f);color:#fff;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:900;font-size:1em;flex-shrink:0}
.contrib-text h3{font-size:0.92em;font-weight:800;color:#01579b;margin-bottom:4px}
.contrib-text p{font-size:0.84em;color:#444;margin:0;line-height:1.6}

/* Results */
.results-grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(150px,1fr));gap:14px;margin:20px 0}
.result-card{background:linear-gradient(135deg,#002171,#0277bd);border-radius:12px;padding:16px;text-align:center}
.result-card .rnum{font-size:1.6em;font-weight:900;color:#fff;line-height:1.1}
.result-card .rdesc{font-size:0.75em;color:#b3e5fc;margin-top:6px;line-height:1.4}

footer{background:#00162b;color:#5baac8;text-align:center;padding:28px 24px;font-size:0.84em}
footer a{color:#80cbc4}

.fig-grid-2{display:grid;grid-template-columns:1fr 1fr;gap:18px;margin-bottom:16px}
@media(max-width:600px){.challenge-grid{grid-template-columns:1fr}.results-grid{grid-template-columns:1fr 1fr}.fig-grid-2{grid-template-columns:1fr}}
</style>
</head>
<body>

<div class="topnav">
  <a href="/">‚Üê Jiajun Fan</a>
  <span>/</span>
  <a href="/publications/">Publications</a>
  <span>/</span>
  <span style="color:#b3e5fc;">ORW-CFM-W2</span>
</div>

<div class="hero">
  <div class="venue-badge">‚úÖ ICLR 2025</div>
  <h1>Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein Regularization</h1>
  <p class="short"><strong>ORW-CFM-W2</strong>: First online RLHF framework for flow matching models ‚Äî no human data, no collapse</p>
  <div class="authors">
    <strong>Jiajun Fan</strong>&thinsp;<sup>1</sup>,
    Shuaike Shen&thinsp;<sup>1</sup>,
    Chaoran Cheng&thinsp;<sup>1</sup>,
    Yuxin Chen&thinsp;<sup>1</sup>,
    Chumeng Liang&thinsp;<sup>2</sup>,
    Ge Liu&thinsp;<sup>1</sup>
  </div>
  <div class="affil"><sup>1</sup>University of Illinois Urbana-Champaign &nbsp;&nbsp; <sup>2</sup>CMU</div>
  <div class="btn-row">
    <a class="btn btn-paper" href="https://openreview.net/forum?id=2IoFFexvuw">üìÑ Paper (OpenReview)</a>
    <a class="btn btn-home" href="/">üè† Author Homepage</a>
  </div>
</div>

<div class="container">

<section>
  <div class="tldr">
    <strong>TL;DR</strong> ‚Äî RL fine-tuning of diffusion models is well-studied, but <strong>flow matching</strong> models (Stable Diffusion 3, Flux) remain underexplored due to unique challenges: no likelihood, policy collapse risk, high compute. ORW-CFM-W2 solves all three ‚Äî online reward-weighting for alignment, Wasserstein-2 regularization for diversity, and a tractable bound that makes it efficient.
  </div>
</section>

<!-- Method Figure -->
<section style="padding:20px 0 30px;border:none">
  <figure style="margin:0">
    <img src="images/method.png" alt="ORW-CFM-W2 Method Architecture"
         style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 4px 24px rgba(0,0,0,0.10)">
    <figcaption style="text-align:center;font-size:0.82em;color:#666;margin-top:12px;line-height:1.6">
      <strong>Figure 1.</strong> ORW-CFM-W2 architecture. Online reward-weighted fine-tuning guides the flow matching model toward high-reward regions, while Wasserstein-2 regularization (W2) prevents policy collapse and maintains generative diversity throughout training.
    </figcaption>
  </figure>
</section>

<section>
  <h2>üöß Why Flow Matching Is Hard to Fine-Tune</h2>
  <div class="challenge-grid">
    <div class="challenge-card">
      <h3>No Likelihood</h3>
      <p>Unlike diffusion models, flow matching has no tractable likelihood ‚Äî standard KL regularization cannot be computed directly.</p>
    </div>
    <div class="challenge-card">
      <h3>Policy Collapse</h3>
      <p>Without diversity constraints, reward maximization causes the model to collapse onto a few high-reward modes, losing generative quality.</p>
    </div>
    <div class="challenge-card">
      <h3>No Human Data</h3>
      <p>Existing fine-tuning methods require expensive human-curated datasets or preference annotations. ORW-CFM-W2 needs neither.</p>
    </div>
  </div>
</section>

<section>
  <h2>üîó RL Perspective on Flow Matching</h2>
  <figure style="margin:0 0 28px">
    <img src="images/rl_overview.png" alt="RL perspective on flow matching fine-tuning"
         style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 4px 20px rgba(0,0,0,0.08)">
    <figcaption style="text-align:center;font-size:0.82em;color:#666;margin-top:10px;line-height:1.6">
      <strong>Figure 2.</strong> Unified RL perspective. ORW-CFM-W2 establishes a formal connection between flow matching fine-tuning and KL-regularized reinforcement learning, enabling principled reward-diversity trade-offs.
    </figcaption>
  </figure>
  
  <h2>‚öôÔ∏è Three Key Contributions</h2>
  <div class="contrib-list">
    <div class="contrib-item">
      <div class="contrib-num">1</div>
      <div class="contrib-text">
        <h3>Online Reward-Weighted Mechanism</h3>
        <p>Integrates RL into flow matching via reward weighting ‚Äî guides the model to prioritize high-reward regions in data space, without requiring reward gradients or filtered datasets.</p>
      </div>
    </div>
    <div class="contrib-item">
      <div class="contrib-num">2</div>
      <div class="contrib-text">
        <h3>Wasserstein-2 Regularization</h3>
        <p>Derives a tractable upper bound for W2 distance in flow matching models ‚Äî prevents policy collapse and maintains generation diversity throughout continuous optimization.</p>
      </div>
    </div>
    <div class="contrib-item">
      <div class="contrib-num">3</div>
      <div class="contrib-text">
        <h3>Unified RL Perspective</h3>
        <p>Establishes a connection between flow matching fine-tuning and traditional KL-regularized RL, enabling controllable reward-diversity trade-offs and deeper understanding of the learning behavior.</p>
      </div>
    </div>
  </div>
</section>

<section>
  <h2>üìä Experiments</h2>
  <div class="results-grid">
    <div class="result-card">
      <div class="rnum">SD3</div>
      <div class="rdesc">Successfully fine-tunes<br>Stable Diffusion 3</div>
    </div>
    <div class="result-card">
      <div class="rnum">0</div>
      <div class="rdesc">Human-collected data<br>required</div>
    </div>
    <div class="result-card">
      <div class="rnum">‚úì</div>
      <div class="rdesc">Spatial understanding<br>&amp; compositional tasks</div>
    </div>
    <div class="result-card">
      <div class="rnum">SOTA</div>
      <div class="rdesc">Orders of magnitude<br>less data than baselines</div>
    </div>
  </div>
  <p style="font-size:0.88em;color:#666;margin-top:8px;">Tasks: target image generation, image compression, text-image alignment. Consistently achieves optimal policy convergence while allowing controllable diversity-reward trade-offs.</p>
</section>

<!-- Visual Results -->
<section>
  <h2>üé® Visual Results on SD3</h2>

  <figure style="margin:0 0 24px">
    <img src="images/positional.png" alt="Positional understanding results"
         style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 4px 20px rgba(0,0,0,0.08)">
    <figcaption style="text-align:center;font-size:0.82em;color:#666;margin-top:10px;line-height:1.6">
      <strong>Figure 2.</strong> Spatial/positional understanding. ORW-CFM-W2 enables SD3 to correctly handle complex positional prompts that the base model fails on.
    </figcaption>
  </figure>

  <div class="fig-grid-2">
    <figure style="margin:0">
      <img src="images/multi_baseline.png" alt="Multi-baseline comparison"
           style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
      <figcaption style="text-align:center;font-size:0.78em;color:#666;margin-top:8px">
        <strong>Figure 3.</strong> Comparison with multiple baselines on target image generation.
      </figcaption>
    </figure>
    <figure style="margin:0">
      <img src="images/multi_reward.png" alt="Multi-reward optimization"
           style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
      <figcaption style="text-align:center;font-size:0.78em;color:#666;margin-top:8px">
        <strong>Figure 4.</strong> Multi-reward optimization across diverse prompts and reward signals.
      </figcaption>
    </figure>
  </div>

  <figure style="margin:24px 0 0">
    <img src="images/semantic.png" alt="Semantic detail enhancement"
         style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 4px 20px rgba(0,0,0,0.08)">
    <figcaption style="text-align:center;font-size:0.82em;color:#666;margin-top:10px;line-height:1.6">
      <strong>Figure 5.</strong> Semantic detail enhancement. Fine-tuned SD3 generates images with richer detail and better semantic correspondence.
    </figcaption>
  </figure>
</section>

<!-- RL Framework -->
<section>
  <h2>üîó Unified RL Framework</h2>
  <figure style="margin:0 0 24px">
    <img src="images/rl_framework.png" alt="RL framework for flow matching"
         style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 4px 24px rgba(0,0,0,0.10)">
    <figcaption style="text-align:center;font-size:0.82em;color:#666;margin-top:10px;line-height:1.6">
      <strong>Figure 6.</strong> Unified RL perspective on flow matching fine-tuning. We establish a formal connection between flow matching optimization and KL-regularized reinforcement learning, providing theoretical grounding for reward-diversity trade-offs.
    </figcaption>
  </figure>
</section>

<!-- Quantitative Analysis -->
<section>
  <h2>üìâ Quantitative Analysis</h2>
  <p style="font-size:0.88em;color:#555;margin-bottom:16px;">Controlled experiments on MNIST and CIFAR demonstrate the reward-diversity trade-off controlled by the W2 regularization strength Œ±:</p>
  <div class="fig-grid-2">
    <figure style="margin:0">
      <img src="images/mnist_reward.png" alt="MNIST reward curves"
           style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
      <figcaption style="text-align:center;font-size:0.78em;color:#666;margin-top:8px">
        <strong>Figure 7.</strong> MNIST reward curves across different Œ± values ‚Äî higher Œ± preserves more diversity.
      </figcaption>
    </figure>
    <figure style="margin:0">
      <img src="images/cifar_reward.png" alt="CIFAR reward tradeoff"
           style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
      <figcaption style="text-align:center;font-size:0.78em;color:#666;margin-top:8px">
        <strong>Figure 8.</strong> CIFAR reward vs. W2 distance Pareto front ‚Äî smooth, controllable trade-off.
      </figcaption>
    </figure>
  </div>
  <div class="fig-grid-2">
    <figure style="margin:0">
      <img src="images/cifar_w2.png" alt="CIFAR W2 tradeoff"
           style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
      <figcaption style="text-align:center;font-size:0.78em;color:#666;margin-top:8px">
        <strong>Figure 9.</strong> W2 distance evolution during training ‚Äî regularization effectively prevents mode collapse.
      </figcaption>
    </figure>
    <figure style="margin:0">
      <img src="images/cifar_text_reward.png" alt="CIFAR text reward"
           style="width:100%;border-radius:12px;border:1px solid #e0e8f0;box-shadow:0 2px 12px rgba(0,0,0,0.06)">
      <figcaption style="text-align:center;font-size:0.78em;color:#666;margin-top:8px">
        <strong>Figure 10.</strong> Text-conditional CIFAR: reward improvement with controlled diversity preservation.
      </figcaption>
    </figure>
  </div>
</section>

<!-- Alpha Control Visualization -->
<section>
  <h2>üéõÔ∏è Alpha Control: Reward‚ÄìDiversity Knob</h2>
  <p style="font-size:0.88em;color:#555;margin-bottom:16px;">The regularization strength Œ± provides a smooth knob between pure reward maximization (Œ±=0) and full diversity preservation (Œ±=1):</p>
  <div style="display:grid;grid-template-columns:repeat(3,1fr);gap:12px;margin-bottom:16px;">
    <figure style="margin:0;text-align:center">
      <img src="images/mnist_alpha0.png" alt="Œ±=0"
           style="width:100%;border-radius:8px;border:1px solid #e0e8f0;">
      <figcaption style="font-size:0.75em;color:#666;margin-top:6px"><strong>Œ± = 0</strong><br>Pure reward (mode collapse)</figcaption>
    </figure>
    <figure style="margin:0;text-align:center">
      <img src="images/mnist_alpha03.png" alt="Œ±=0.3"
           style="width:100%;border-radius:8px;border:1px solid #e0e8f0;">
      <figcaption style="font-size:0.75em;color:#666;margin-top:6px"><strong>Œ± = 0.3</strong><br>Balanced (recommended)</figcaption>
    </figure>
    <figure style="margin:0;text-align:center">
      <img src="images/mnist_alpha08.png" alt="Œ±=0.8"
           style="width:100%;border-radius:8px;border:1px solid #e0e8f0;">
      <figcaption style="font-size:0.75em;color:#666;margin-top:6px"><strong>Œ± = 0.8</strong><br>High diversity preserved</figcaption>
    </figure>
  </div>
  <div style="display:grid;grid-template-columns:repeat(3,1fr);gap:12px;">
    <figure style="margin:0;text-align:center">
      <img src="images/dog_base.png" alt="SD3 Base"
           style="width:100%;border-radius:8px;border:1px solid #e0e8f0;">
      <figcaption style="font-size:0.75em;color:#666;margin-top:6px"><strong>SD3 Base</strong><br>Before fine-tuning</figcaption>
    </figure>
    <figure style="margin:0;text-align:center">
      <img src="images/dog_05.png" alt="Œ±=0.5"
           style="width:100%;border-radius:8px;border:1px solid #e0e8f0;">
      <figcaption style="font-size:0.75em;color:#666;margin-top:6px"><strong>Œ± = 0.5</strong><br>Balanced fine-tuning</figcaption>
    </figure>
    <figure style="margin:0;text-align:center">
      <img src="images/dog_10.png" alt="Œ±=1.0"
           style="width:100%;border-radius:8px;border:1px solid #e0e8f0;">
      <figcaption style="font-size:0.75em;color:#666;margin-top:6px"><strong>Œ± = 1.0</strong><br>Full reward optimization</figcaption>
    </figure>
  </div>
</section>

<section>
  <h2>üìÖ Publication Journey</h2>
  <div style="position:relative;padding-left:24px;margin:8px 0">
    <div style="position:absolute;left:8px;top:0;bottom:0;width:2px;background:linear-gradient(to bottom,#0277bd,#80deea)"></div>
    <div style="position:relative;margin-bottom:18px;padding-left:4px">
      <div style="position:absolute;left:-20px;top:5px;width:12px;height:12px;background:#0277bd;border-radius:50%;border:2px solid #fff;box-shadow:0 0 0 2px #0277bd"></div>
      <div style="font-size:.78em;font-weight:800;color:#0277bd;text-transform:uppercase;letter-spacing:.05em">Oct 2024</div>
      <div style="font-weight:700;font-size:.93em">Submitted to ICLR 2025</div>
      <div style='font-size:.83em;color:#555'>Submitted to The Thirteenth International Conference on Learning Representations.</div>
    </div>
    <div style="position:relative;margin-bottom:18px;padding-left:4px">
      <div style="position:absolute;left:-20px;top:5px;width:12px;height:12px;background:#1b5e20;border-radius:50%;border:2px solid #fff;box-shadow:0 0 0 2px #1b5e20"></div>
      <div style="font-size:.78em;font-weight:800;color:#1b5e20;text-transform:uppercase;letter-spacing:.05em">Feb 2025</div>
      <div style="font-weight:700;font-size:.93em">‚úÖ Accepted at ICLR 2025 (Poster)</div>
      <div style='font-size:.83em;color:#555'>Accepted as a poster. <a href="https://openreview.net/forum?id=2IoFFexvuw">OpenReview</a></div>
    </div>
    <div style="position:relative;margin-bottom:18px;padding-left:4px">
      <div style="position:absolute;left:-20px;top:5px;width:12px;height:12px;background:#1b5e20;border-radius:50%;border:2px solid #fff;box-shadow:0 0 0 2px #1b5e20"></div>
      <div style="font-size:.78em;font-weight:800;color:#1b5e20;text-transform:uppercase;letter-spacing:.05em">Apr 2025</div>
      <div style="font-weight:700;font-size:.93em">Presented at ICLR 2025 ¬∑ Singapore</div>
      
    </div>
  </div>
</section>

<section>
  <h2>üìñ BibTeX</h2>
  <pre style="background:#e1f5fe;border-radius:8px;padding:16px;font-size:0.82em;overflow-x:auto;border:1px solid #b3e5fc;"><code>@inproceedings{fan2025online,
  title={Online Reward-Weighted Fine-Tuning of Flow Matching
         with Wasserstein Regularization},
  author={Jiajun Fan and Shuaike Shen and Chaoran Cheng
          and Yuxin Chen and Chumeng Liang and Ge Liu},
  booktitle={The Thirteenth International Conference on
             Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=2IoFFexvuw}
}</code></pre>
</section>

</div>

<footer>
  <p>ORW-CFM-W2 ¬∑ ICLR 2025 &nbsp;|&nbsp; <a href="/">Jiajun Fan</a> ¬∑ UIUC &nbsp;|&nbsp; <a href="https://openreview.net/forum?id=2IoFFexvuw">Paper</a></p>
</footer>
</body>
</html>
